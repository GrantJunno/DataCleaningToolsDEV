{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e79fa1-f0af-41f7-ab20-5bf632d6342c",
   "metadata": {},
   "source": [
    "# TEMPORARY VARIABLE ASSIGNMENTS FOR DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcffe56c-12c8-4d0f-83da-bbdff411095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR TESTING\n",
    "dataset = goog_data\n",
    "check_var_col = 0\n",
    "print('You\\'re still assigning testing variables in GTJ_DatasetCleaningHelpers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa78716-e36f-4db6-80e4-47ce35efd32d",
   "metadata": {},
   "source": [
    "## Slice datasets based on indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ced1dbc5-9099-4984-a70e-2ad5587b792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entries_by_indices(dataset, remove_indices):\n",
    "    '''\n",
    "    #Function that will take a dataset and return all entries except for those with indices listed in remove_indices\n",
    "    #RETURNS:\n",
    "    ##dataset - Same dataset that was an input, but WITHOUT the entries associeted with remove_indices indices\n",
    "    #INPUT VARS:\n",
    "    ##dataset - open()ed and reader()ed csv file (or other source providing a list of lists of rows)\n",
    "    ##remove_indices - list (or anything convertable to list type) of indices to be removed from dataset\n",
    "    '''\n",
    "    #Check if any of the remove_indices are negative\n",
    "    #if so, throw an exception\n",
    "    #TODO: handle this by conversion but I don't want to right now\n",
    "    if any([x < 0 for x in remove_indices]):\n",
    "        raise Exception(\"Don't give me negative indices to remove. Rude. Maybe I'll allow this some day, but not this day.\")\n",
    "        \n",
    "    #Check if remove_indices is a list. \n",
    "    #TODO: DONE Could combine these as list(a_list) doesn't change 'a_list'\n",
    "    # if type(remove_indices) is list:\n",
    "    #     #If so, make it a set\n",
    "    #     remove_indices = set(remove_indices)\n",
    "    #Or if it's not a list and not already a set\n",
    "    #elif type(remove_indices) is not set:\n",
    "    if type(remove_indices) is not set:\n",
    "        remove_indices = set(list(remove_indices))\n",
    "        #raise Exception(\"remove_indices needs to be a list or a set\")\n",
    "    \n",
    "    #Drop entries via list comprehension\n",
    "    dataset = [i for j, i in enumerate(dataset) if j not in remove_indices]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b552950f-557f-412c-8e46-3dddb439882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_entries_by_indices(dataset, return_indices):\n",
    "    '''\n",
    "    #Function that will take a dataset and return all entries with indices listed in return_indices\n",
    "    #RETURNS:\n",
    "    ##dataset - Same dataset that was an input, but with ONLY the entries associeted with return_indices indices\n",
    "    #INPUT VARS:\n",
    "    ##dataset - open()ed and reader()ed csv file (or other source providing a list of lists of rows)\n",
    "    ##remove_indices - list (or anything convertable to list type) of indices to be returned from dataset\n",
    "    '''\n",
    "    #Check if any of the return_indices are negative\n",
    "    #if so, throw an exception\n",
    "    #TODO: handle this by conversion but I don't want to right now\n",
    "    if any([x < 0 for x in return_indices]):\n",
    "        raise Exception(\"Don't give me negative indices to return. Rude. Maybe I'll allow this some day, but not this day.\")\n",
    "        \n",
    "    #Check if return_indices is a list. \n",
    "    #TODO: DONE Could combine these as list(a_list) doesn't change 'a_list'\n",
    "    #if type(return_indices) is list:\n",
    "        ##If so, make it a set\n",
    "        #return_indices = set(return_indices)\n",
    "    #Or if it's not a list and not already a set\n",
    "    #eliftype(return_indices) is not set:\n",
    "    if type(return_indices) is not set:\n",
    "        return_indices = set(list(return_indices))\n",
    "        #raise Exception(\"remove_indices needs to be a list or a set\")\n",
    "    \n",
    "    #Keep entries via list comprehension\n",
    "    dataset = [i for j, i in enumerate(dataset) if j in return_indices]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844a34b-1fa2-4624-bd4d-78eae96e58ee",
   "metadata": {},
   "source": [
    "## Print out a slice of a dataset for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a94837eb-36ea-4f35-9842-a095c9ea369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def explore_data(dataset, start, end, rows_and_columns=False):\n",
    "    '''\n",
    "    #Define a useful function for exploring (displaying) a slice of a given dataset\n",
    "    #RETURNS: \n",
    "    ##Nothing\n",
    "    #INPUT VARS:\n",
    "    ##dataset - open()ed and reader()ed csv file (or other source providing a list of lists of rows)\n",
    "    ##start - First row to include in the slice (0-indexing)\n",
    "    ##end - Last row to include in the slice (1-indexing because python I guess)\n",
    "    ##rows_and_columns - True prints out numbers of rows/columns, False (default) doesn't\n",
    "    '''\n",
    "    dataset_slice = list(dataset[start:end])\n",
    "    for row in dataset_slice:\n",
    "        print(row)\n",
    "        print('\\n') # Add an empty line after each row\n",
    "    \n",
    "    if rows_and_columns:\n",
    "        print('Number of rows:', len(dataset))\n",
    "        print('Number of columns: ', len(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae8f804-d182-40d3-94cd-8eefb885bac8",
   "metadata": {},
   "source": [
    "## Various functions for finding and handling duplicate entries in a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d740de1-d3ef-48d0-9a2a-ae01f8d50f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_duplicate_entries(dataset, check_var_col):\n",
    "    '''\n",
    "    #Define another useful function for detecting duplicate entries in a dataset\n",
    "    #COMPARES app names to determine if entries are duplicates. Marks second and later entries with the same name as one that has already occurred as duplicates.\n",
    "    #RETURNS:\n",
    "    ##unique_entries - [LIST] of unique app entries' rows [CAN BE USED AS A DATASET WITH NO DUPLICATES]\n",
    "    ##duplicate_entries - [LIST] of the duplicate app entries' rows\n",
    "    ##duplicate_names_and_indices - [LIST] of the app names and indices of duplicate app entries in the UNIQUE dataset\n",
    "    #INPUT VARS:\n",
    "    ##dataset - open()ed and reader()ed csv file (or other source providing a list of lists of rows)\n",
    "    ##check_var_col - Which column in the dataset corresponds to app name (or other desired duplication check variable)\n",
    "    ###NOTE: 0 for goog_data, 1 for app_data\n",
    "    '''\n",
    "    unique_entries = [] #first and potentially only copy of an app entry\n",
    "    unique_entry_indices = {} #Tracks indices of unique entries in the original AND unique datasets (in that order)\n",
    "    unique_entry_names = [] #Just tracks entry names for comparison\n",
    "    duplicate_entries = [] #second and further copies of app entries\n",
    "    duplicate_names_and_indices = [] #Just the app name and the index of the corresponding index in unique_entries\n",
    "    #duplicate_corresponding_row = [] #Copy of the row that ended up in unique_entries\n",
    "\n",
    "    for index,app in enumerate(dataset):\n",
    "        app_name = app[check_var_col]\n",
    "        if app_name not in unique_entry_names:\n",
    "            unique_entry_names.append(app_name)\n",
    "            unique_entries.append(app)\n",
    "            unique_entry_indices[app_name] = [index, len(unique_entries)-1]\n",
    "        else:\n",
    "            duplicate_entries.append(app)\n",
    "            #duplicate_corresponding_row.append(dataset[unique_entry_indices[app_name]])\n",
    "            duplicate_names_and_indices.append([app_name, unique_entry_indices[app_name]])#dataset[unique_entry_indices[app_name][0]]])\n",
    "    \n",
    "    return unique_entries, duplicate_entries, duplicate_names_and_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdcea8cc-6942-46d4-b414-23f798462a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def smart_split_duplicate_entries(dataset, check_var_col, ignore_cols=[]):\n",
    "    '''\n",
    "    #Compares duplicate entries (matched by app name) to corresponding unique ones and returns only entries that are not whole-row matches \n",
    "    #(AKA names might be duplicates but some other element in the row doesn't line up)\n",
    "    '''\n",
    "    unique_entries, duplicate_entries, duplicate_names_and_indices = split_duplicate_entries(dataset, check_var_col)\n",
    "    interesting_duplicates = [] #list populated with entries that were marked duplicte by name but do NOT perfectly match the corresponding unique row\n",
    "    interesting_duplicate_names_and_indices = []\n",
    "    for index,app in enumerate(duplicate_entries[0:5]):\n",
    "        if not app == unique_entries[duplicate_names_and_indices[index][1][1]]:\n",
    "            interesting_duplicates.append(app)\n",
    "            #app_name = app[check_var_col]\n",
    "            interesting_duplicate_names_and_indices.append(duplicate_names_and_indices[index])\n",
    "            \n",
    "    return unique_entries, interesting_duplicates, interesting_duplicate_names_and_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e3aeb8-d01c-42aa-ace6-2d9676aa948e",
   "metadata": {},
   "source": [
    "## This is some testing stuff/WIP below here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26597f99-b122-489f-baf5-0024845c3e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def duplicate_entry_differences(dataset, check_var_col):\n",
    "#     '''\n",
    "#     #Reviews duplicate pings in the dataset, RETURNS EITHER LOCATIONS OR CONTENTS OF MISMATCHED LIST ELEMENTS\n",
    "\n",
    "\n",
    "#     '''\n",
    "    \n",
    "unique_entries, interesting_duplicates, interesting_duplicate_names_and_indices = smart_split_duplicate_entries(dataset, check_var_col)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718272b-e558-4d58-be38-cf49d244232c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8381f8a0-2661-490f-9925-31a157dae10e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m subU \u001b[38;5;241m=\u001b[39m [unique_entries[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43minteresting_duplicate_names_and_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "subU = [unique_entries[i] for i in interesting_duplicate_names_and_indices[0:5][1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52c7bb5-0b5a-4d53-9e51-d5ed0cf29c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mannequin Challenge', [2949, 2949]], ['VR Roller Coaster', [4443, 4443]]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interesting_duplicate_names_and_indices[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "43430340-447c-42e1-893a-f1ff2b448832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver']\n",
      "\n",
      "\n",
      "['Photo Editor & Candy Camera & Grid & ScrapBook', 'ART_AND_DESIGN', '4.1', '159', '19M', '10,000+', 'Free', '0', 'Everyone', 'Art & Design', 'January 7, 2018', '1.0.0', '4.0.3 and up']\n",
      "\n",
      "\n",
      "['Coloring book moana', 'ART_AND_DESIGN', '3.9', '967', '14M', '500,000+', 'Free', '0', 'Everyone', 'Art & Design;Pretend Play', 'January 15, 2018', '2.0.0', '4.0.3 and up']\n",
      "\n",
      "\n",
      "['U Launcher Lite â€“ FREE Live Cool Themes, Hide Apps', 'ART_AND_DESIGN', '4.7', '87510', '8.7M', '5,000,000+', 'Free', '0', 'Everyone', 'Art & Design', 'August 1, 2018', '1.2.4', '4.0.3 and up']\n",
      "\n",
      "\n",
      "['Sketch - Draw & Paint', 'ART_AND_DESIGN', '4.5', '215644', '25M', '50,000,000+', 'Free', '0', 'Teen', 'Art & Design', 'June 8, 2018', 'Varies with device', '4.2 and up']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explore_data(goog_data, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9313bcb0-5789-4ad8-b365-17d1233cf19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10842"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(goog_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d04ccc9-4d3e-466c-855c-ecb88b97866d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7198"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f7083fd-eec6-4ab4-a38c-64f85656c437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9660"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(goog_data_nodup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a9895c-8326-45f1-94bf-c947fb053845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7196"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce64f223-11c8-4585-b480-27d957349ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
